#!/usr/bin/env python3

import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image, CameraInfo # Rimosso PointCloud2 se non usato
from geometry_msgs.msg import Pose, PoseArray # Rimosso Point, Header se non usati direttamente
from std_msgs.msg import String
from cv_bridge import CvBridge
import cv2
import numpy as np
from ultralytics import YOLO
import tf2_ros
import tf2_geometry_msgs
# from geometry_msgs.msg import TransformStamped # Non strettamente necessario
# import sensor_msgs_py.point_cloud2 as pc2 # Rimosso se non usato
# from scipy.spatial.transform import Rotation # Rimosso se non usato per orientamento

class VisionNode(Node):
    def __init__(self):
        super().__init__('vision_node')
        
        # Initialize YOLO model
        # self.model = YOLO('yolov8n.pt') # Per testare con oggetti COCO
        # Sostituisci con il percorso al tuo modello custom quando pronto:
        # from ament_index_python.packages import get_package_share_directory
        # import os
        # package_share_directory = get_package_share_directory('ur5_eigen_motion')
        # model_path = os.path.join(package_share_directory, 'models', 'yolo', 'best.pt')
        # self.model = YOLO(model_path)
        self.model = YOLO('yolov8n.pt') # Torno a yolov8n.pt per ora
        
        self.bridge = CvBridge()
        self.tf_buffer = tf2_ros.Buffer()
        self.tf_listener = tf2_ros.TransformListener(self.tf_buffer, self)
        
        self.camera_matrix = None
        self.dist_coeffs = None
        self.camera_frame = "camera_rgb_frame" # Confermato dall'output di CameraInfo
        self.base_frame = "base_link"
        
        self.image_sub = self.create_subscription(
            Image, '/camera/image_raw/image', self.image_callback, 10)
        self.depth_sub = self.create_subscription(
            Image, '/camera/image_raw/depth_image', self.depth_callback, 10)
        self.camera_info_sub = self.create_subscription(
            CameraInfo, '/camera/image_raw/camera_info', self.camera_info_callback, 10)
        
        self.pose_array_pub = self.create_publisher(PoseArray, '/detected_objects', 10)
        self.classification_pub = self.create_publisher(String, '/object_classification', 10)
        self.debug_image_pub = self.create_publisher(Image, '/vision_debug', 10)
        
        self.latest_color_image = None
        self.latest_depth_image = None
        
        self.class_mapping = {
            # Questo è un ESEMPIO per COCO. Devi adattarlo al TUO modello custom.
            # Se yolov8n.pt rileva uno 'stop sign' (ID 11 in COCO), e vuoi chiamarlo 'prism'
            # 11: 'prism', 
            # Se rileva 'sports ball' (ID 32) e vuoi chiamarlo 'sphere'
            # 32: 'sphere',
            # Se rileva 'bottle' (ID 39) e vuoi chiamarlo 'cylinder'
            # 39: 'cylinder',
            # Per ora, metto un mapping di esempio che potrebbe corrispondere ai tuoi oggetti
            # se il tuo modello custom avesse questi ID.
            0: 'cube', 
            1: 'cylinder',
            2: 'sphere',
            3: 'prism',
             # Aggiunto per il log che hai visto
        }
        
        self.timer = self.create_timer(0.1, self.process_images) # Timer più frequente per reattività
        self.get_logger().info("Vision Node initialized with YOLO")

    def camera_info_callback(self, msg):
        self.camera_matrix = np.array(msg.k).reshape(3, 3)
        self.dist_coeffs = np.array(msg.d)

    def image_callback(self, msg):
        try:
            self.latest_color_image = self.bridge.imgmsg_to_cv2(msg, "bgr8")
        except Exception as e:
            self.get_logger().error(f"Error converting color image: {e}")

    def depth_callback(self, msg):
        try:
            # Gestisce correttamente 32FC1 e 16UC1 basandosi sull'encoding
            if msg.encoding == "32FC1":
                self.latest_depth_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding="passthrough")
            elif msg.encoding == "16UC1":
                self.latest_depth_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding="passthrough")
            else:
                self.get_logger().error(f"Unsupported depth image encoding: {msg.encoding}")
                self.latest_depth_image = None
                return
        except Exception as e:
            self.get_logger().error(f"Error converting depth image: {e}")
            self.latest_depth_image = None

    def process_images(self):
        if self.latest_color_image is None or self.latest_depth_image is None or self.camera_matrix is None:
            return # Attendi che tutti i dati siano disponibili
            
        # Lavora su copie per evitare race conditions se i callback aggiornano i membri
        current_color_image = self.latest_color_image.copy()
        current_depth_image = self.latest_depth_image.copy()
            
        detections = self.detect_objects(current_color_image)
        
        if not detections:
            # Opzionale: pubblica immagine di debug vuota se vuoi vedere il feed
            # self.publish_debug_image(current_color_image)
            return
            
        # Passa current_depth_image a detections_to_poses
        poses, classifications = self.detections_to_poses(detections, current_depth_image)
        
        if poses:
            self.publish_poses(poses)
            self.publish_classifications(classifications)
            
        # Disegna sull'immagine a colori corrente e pubblicala
        debug_image = self.draw_detections(current_color_image, detections)
        self.publish_debug_image(debug_image)

    def detect_objects(self, image):
        try:
            # Abbassato conf per test, aggiusta secondo necessità
            results = self.model(image, conf=0.2, iou=0.4) 
            detections = []
            
            for r in results:
                boxes = r.boxes
                if boxes is not None:
                    for box in boxes:
                        xyxy = box.xyxy[0].cpu().numpy()
                        conf = box.conf[0].cpu().numpy()
                        cls_id = int(box.cls[0].cpu().numpy()) # Salva come cls_id
                        
                        center_x = int((xyxy[0] + xyxy[2]) / 2)
                        center_y = int((xyxy[1] + xyxy[3]) / 2)
                        
                        detection = {
                            'bbox': xyxy,
                            'confidence': conf,
                            'class_id': cls_id, # Usa class_id
                            'center': (center_x, center_y),
                            # Usa un fallback più informativo se l'ID non è nel mapping
                            'class_name': 'sphere'#self.class_mapping.get(cls_id, 'sphere')
                        }
                        detections.append(detection)
            return detections
        except Exception as e:
            self.get_logger().error(f"Error in YOLO detection: {e}")
            return []

    # Modificato per accettare depth_img come argomento
    def detections_to_poses(self, detections, depth_img):
        poses = []
        classifications = []
        
        for i, detection in enumerate(detections):
            try:
                center_x, center_y = detection['center']
                
                # Usa l'immagine di profondità passata come argomento
                depth_value = self.get_depth_at_pixel(center_x, center_y, depth_img)
                

                if depth_value == 0.0: # Filtro per profondità invalida
                    self.get_logger().warn(f"Det {i}: Invalid depth (0.0). Skipping.")
                    continue
                
                depth_m = 0.0
                if depth_img.dtype == np.uint16:
                    depth_m = depth_value / 1000.0  # mm to m
                elif depth_img.dtype == np.float32:
                    depth_m = depth_value # Già in metri
                else:
                    self.get_logger().error(f"Det {i}: Unknown depth image dtype: {depth_img.dtype}. Skipping.")
                    continue
                
                # Aggiungi un filtro per profondità irragionevoli
                if not (0.1 < depth_m < 5.0): # Esempio: tra 10cm e 5m
                    self.get_logger().warn(f"Det {i}: Depth_m {depth_m:.4f} out of reasonable range. Skipping.")
                    continue
                

                
                camera_point = self.pixel_to_camera_coords(center_x, center_y, depth_m)
                if camera_point is None: continue # pixel_to_camera_coords ora può restituire None

                
                world_pose = self.transform_to_base_frame(camera_point)
                
                if world_pose:
                    poses.append(world_pose)
                    classifications.append({
                        'object_id': f'object_{i}', # ID locale al batch
                        'class_name': 'sphere'#detection['class_name']
                    })
            except Exception as e:
                # Stampa l'eccezione completa per un debug migliore
                self.get_logger().error(f"Error converting detection {i} to pose: {e}", exc_info=True)
                continue
        return poses, classifications

    # Modificato per accettare current_depth_img come argomento
    def get_depth_at_pixel(self, x, y, current_depth_img):
        if current_depth_img is None:
            return 0.0 # Ritorna float
            
        height, width = current_depth_img.shape
        if 0 <= x < width and 0 <= y < height:
            roi_size = 5
            x_start = max(0, x - roi_size // 2)
            # Corretto per evitare off-by-one e assicurare dimensione ROI
            x_end = min(width, x_start + roi_size) 
            y_start = max(0, y - roi_size // 2)
            y_end = min(height, y_start + roi_size)
            
            # Assicurati che la ROI non sia vuota se x_start o y_start sono vicini ai bordi
            if y_start >= y_end or x_start >= x_end:
                return 0.0

            roi = current_depth_img[y_start:y_end, x_start:x_end]
            
            if current_depth_img.dtype == np.uint16:
                valid_depths = roi[roi > 0] 
            elif current_depth_img.dtype == np.float32:
                # Filtro più robusto per float, adatta i limiti se necessario
                valid_depths = roi[np.logical_and(roi > 0.01, roi < 10.0)] 
            else:
                self.get_logger().warn(f"Unknown dtype in get_depth_at_pixel: {current_depth_img.dtype}")
                return 0.0

            if valid_depths.size > 0:
                return float(np.median(valid_depths)) # Ritorna float
        return 0.0 # Ritorna float

    def pixel_to_camera_coords(self, u, v, depth_m): # Rinominato depth in depth_m per chiarezza
        if self.camera_matrix is None:
            self.get_logger().warn("Camera matrix not available for pixel_to_camera_coords.")
            return None # Restituisci None per indicare fallimento
            
        fx = self.camera_matrix[0, 0]
        fy = self.camera_matrix[1, 1]
        cx = self.camera_matrix[0, 2]
        cy = self.camera_matrix[1, 2]
        
        if fx == 0 or fy == 0: # Controllo divisione per zero
            self.get_logger().error("Invalid focal length (fx or fy is zero) in camera_matrix.")
            return None

        x_cam = (u - cx) * depth_m / fx
        y_cam = (v - cy) * depth_m / fy
        z_cam = depth_m # z nella camera è la profondità
        
        return np.array([x_cam, y_cam, z_cam])

    def transform_to_base_frame(self, camera_point): # camera_point è già un array numpy
        # Rinominato il log per usare la variabile corretta
        self.get_logger().info(f"Attempting to transform point from '{self.camera_frame}' to '{self.base_frame}': {camera_point}")
        try:
            point_stamped_camera = tf2_geometry_msgs.PointStamped()
            point_stamped_camera.header.frame_id = self.camera_frame
            point_stamped_camera.header.stamp = self.get_clock().now().to_msg()
            point_stamped_camera.point.x = float(camera_point[0])
            point_stamped_camera.point.y = float(camera_point[1])
            point_stamped_camera.point.z = float(camera_point[2])
            
            transform = self.tf_buffer.lookup_transform(
                self.base_frame, 
                self.camera_frame, 
                rclpy.time.Time() # Ultima trasformazione disponibile
            )
            # self.get_logger().info(f"Found transform: Translation: {transform.transform.translation}, Rotation: {transform.transform.rotation}")
            
            transformed_point_msg = tf2_geometry_msgs.do_transform_point(point_stamped_camera, transform)
            # Rinominato il log per usare la variabile corretta
            self.get_logger().info(f"Transformed point in base_frame: ({transformed_point_msg.point.x:.3f}, {transformed_point_msg.point.y:.3f}, {transformed_point_msg.point.z:.3f})")
            
            pose = Pose()
            pose.position.x = transformed_point_msg.point.x
            pose.position.y = transformed_point_msg.point.y
            pose.position.z = transformed_point_msg.point.z
            pose.orientation.w = 1.0
            
            return pose
            
        except (tf2_ros.LookupException, tf2_ros.ConnectivityException, tf2_ros.ExtrapolationException) as e:
            self.get_logger().warn(f"TF2 transform error from '{self.camera_frame}' to '{self.base_frame}': {e}")
            return None
        except Exception as e:
            self.get_logger().error(f"Unexpected error in transform_to_base_frame: {e}", exc_info=True)
            return None

    def publish_poses(self, poses):
        pose_array = PoseArray()
        pose_array.header.frame_id = self.base_frame
        pose_array.header.stamp = self.get_clock().now().to_msg()
        pose_array.poses = poses
        self.pose_array_pub.publish(pose_array)

    def publish_classifications(self, classifications):
        for cls_info in classifications:
            msg = String()
            msg.data = f"{cls_info['object_id']}:{cls_info['class_name']}"
            self.classification_pub.publish(msg)

    def draw_detections(self, image, detections):
        for detection in detections:
            bbox = detection['bbox']
            conf = detection['confidence']
            class_name = detection['class_name'] # Questo viene dal mapping
            
            cv2.rectangle(image, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), (0, 255, 0), 2)
            label = f"{class_name}({detection['class_id']}): {conf:.2f}" # Aggiunto ID classe per debug
            cv2.putText(image, label, (int(bbox[0]), int(bbox[1]) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
            center = detection['center']
            cv2.circle(image, center, 5, (255, 0, 0), -1)
        return image

    def publish_debug_image(self, image):
        try:
            img_msg = self.bridge.cv2_to_imgmsg(image, "bgr8")
            self.debug_image_pub.publish(img_msg)
        except Exception as e:
            self.get_logger().error(f"Error publishing debug image: {e}")

def main(args=None):
    rclpy.init(args=args)
    node = VisionNode()
    try:
        rclpy.spin(node)
    except KeyboardInterrupt:
        pass
    finally:
        if rclpy.ok(): # Controlla se rclpy è ancora ok prima di chiamare destroy_node
            node.destroy_node()
            rclpy.shutdown()

if __name__ == '__main__':
    main()